[{"authors":["admin"],"categories":null,"content":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026amp; processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems. Carlos joined UC Santa Cruz in 2004, after five years at Netapp working on network-intermediaries and storage systems. In 2005 he co-founded and became a key mentor on Sage Weil’s Ceph project. In 2008 Carlos became a member of the computer science faculty at UC Santa Cruz and has graduated nine Ph.D. students since. Carlos graduated with a M.S. and Ph.D. in Computer Science from University of Colorado at Boulder.\nHis work is funded by nonprofits, government, and industry, including NSF OAC-2226407, the Alfred P. Sloan Foundation G-2021-16957, DOE ASCR DE-NA0003525 (FWP 20-023266, subcontractor of Sandia National Labs), NSF OAC-1836650, NSF CNS-1764102, NSF CNS-1705021, DOE ASCR DE-SC0016074, NSF OAC-1450488, and CROSS\nFor more details, see his homepage and his vitae.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://skyhookdm.github.io/author/carlos-maltzahn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/carlos-maltzahn/","section":"authors","summary":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026 processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems.","tags":null,"title":"Carlos Maltzahn","type":"authors"},{"authors":["jayjeet.chakraborty"],"categories":null,"content":"Towards Faster Columnar Data Transport using RDMA\nAbstract: The amount of data stored in data centers worldwide is increasing faster than ever. Much of this data is stored to process, analyze, and extract valuable insights. Primarily, large datasets are stored in dumb storage servers inside data centers. To perform analysis and compute on this data, it must first be transported to compute servers, usually with big DRAMs and huge processing capabilities, where the data is processed and result sets sent to the client. Traditional data transport frameworks use TCP/IP as their underlying protocol and therefore require the users to provide with a single contiguous buffer for transfer to the framework. We observe that this approach adds a severe overhead when transferring batches of columnar records. Columnar tables consist of several buffers scattered in the memory, each representing a particular column and its metadata. Serializing these buffers into a single buffer requires doing multiple memory copies. To avoid this serialization overhead while transfer columnar batches over the wire, we propose using RDMA as a potential solution to this problem. In this paper, we explore transporting columnar result sets over RDMA from a server to a client and compare this approach with a state-of-the-art TCP/IP based transport framework. We show that using RDMA for columnar data transport provides up to 2x higher throughput.\nBio: Jayjeet Chakraborty is a PhD student at the University of California, Santa Cruz. He is advised by Carlos Maltzahn. His research interests include Data management and Database systems, Computational storage systems, Distributed systems, and Systems in general. For his PhD, he is working with Argonne National Labs on building a hardware-accelerated columnar data transport frameworks leveraging RDMA.\nContact: Jayjeet Chakraborty; e-mail: jayjeetc@ucsc.edu\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b97a8deeea47982adec66d16c00b787","permalink":"https://skyhookdm.github.io/author/jayjeet-chakraborty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jayjeet-chakraborty/","section":"authors","summary":"Towards Faster Columnar Data Transport using RDMA\nAbstract: The amount of data stored in data centers worldwide is increasing faster than ever. Much of this data is stored to process, analyze, and extract valuable insights.","tags":null,"title":"Jayjeet Chakraborty","type":"authors"},{"authors":["matthew.curry"],"categories":null,"content":"An HPC-Oriented Runtime Environment for Enabling Computational Storage\nAbstract: There are at least two methods for enabling computational storage workflows on embedded storage hardware. The first revolutionary path is to port applications to relatively lightweight storage abstractions that allow simple software stacks. A second evolutionary path is to enable existing storage abstractions by porting existing libraries and utilities to computational storage hardware. While the evolutionary path is a good stop-gap measure until applications can be prepared, porting significant dependency chains for complex libraries to the required interesting architectures can be a significant hurdle.\nIn this talk, I will describe the Advanced Tri-lab Software Environment (ATSE). ATSE is a traditional HPC software stack for testbed systems. As a full-featured stack, it is able to provide complex I/O libraries such as HDF5, NetCDF, CGNS, Exodus, and others. Crucially, ATSE has been demonstrated to port to many variants of Arm and RISC-V processors, allowing use on low-power hardware often found in computational storage devices. Along with the stack itself, this talk will touch on techniques and facilities allowing ATSE’s high levels of portability.\nBio: Matthew Curry has a wide variety of research interests, mostly pertaining directly to data storage in supercomputing environments, including parallel and distributed storage systems, erasure coding and fault tolerance, low-level (block or object) storage devices, and heterogeneous computing (e.g., GPUs, accelerators). He graduated with a Ph.D. in Computer Science at University of Alabama at Birmingham under the advisement of Dr. Anthony Skjellum.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d5ba6a3324f6e6276c80c4e4342b24ff","permalink":"https://skyhookdm.github.io/author/matthew-curry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthew-curry/","section":"authors","summary":"An HPC-Oriented Runtime Environment for Enabling Computational Storage\nAbstract: There are at least two methods for enabling computational storage workflows on embedded storage hardware. The first revolutionary path is to port applications to relatively lightweight storage abstractions that allow simple software stacks.","tags":null,"title":"Matthew Curry","type":"authors"},{"authors":["pankaj.mehra"],"categories":null,"content":"Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface\nAbstract: Simple enough to describe in one sentence, the theory of operation of XPI, an interface that targets acceleration starting to show up in and near memory and storage devices, revolves around three core concepts. First, an infrastructure abstraction that uniformizes disaggregated infrastructure. Second, a new abstraction for comprehending an application’s data spanning the full richness of modern memory and storage systems. Third, an abstraction capable of supporting the offloading of interdependent parallel computations with set-valued dependencies. The talk is motivated through the upcoming upheaval of memory hierarchy in data centers and concludes with a call to action for industry and academia participants alike.\nBio: Pankaj Mehra is President and CEO of Elephance Memory. A contributor to InfiniBand 1.0 spec and an inventor of the first RDMA persistent memory devices and filesystems, Pankaj is a systems and software practitioner who conducts his research at the intersection of infrastructure and intelligence. He is a co-author/co-editor/co-inventor on 3 books, and over 100 papers and patents, and a builder of systems that have won recognition from NASA, Sandia National Labs, and Samsung Electronics, among others. He was a VP of Storage Pathfinding at Samsung, Senior Fellow and VP at SanDisk and Western Digital, WW CTO of Fusion-io, and Distinguished Technologist at HP Labs. Pankaj has held faculty positions at IIT Delhi and UC Santa Cruz.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ac06de0c8dee491d8b860b0acc0cb6e3","permalink":"https://skyhookdm.github.io/author/pankaj-mehra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pankaj-mehra/","section":"authors","summary":"Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface\nAbstract: Simple enough to describe in one sentence, the theory of operation of XPI, an interface that targets acceleration starting to show up in and near memory and storage devices, revolves around three core concepts.","tags":null,"title":"Pankaj Mehra","type":"authors"},{"authors":["philip.kufeldt"],"categories":null,"content":"Kinetic Open Storage Project Update\nAbstract: Prior to departing Seagate I was able to open source the remaining components of Seagate’s Kinetic implementation, namely their server, kineticd. This kinetic server implementation was used inside their production 4T and 8T Kinetic HDD offerings. An entire Kinetic research vehicle in source form is now available for research, from client to server. This presentation will go over this open source.\nBio: Philip Kufeldt’s last role was a Technologist in Seagate Research. He focuses on computational storage using smart HDDs. He has 30 years of experience in storage, software, and systems. Before Seagate, Philip was at Toshiba, creating new smart media devices and founding the Kinetic Open Storage Project; additionally he has spent time at UCSC, Marvell, Parascale, VERITAS Software, Sun Microsystems, and IBM; and he has founded several storage oriented startups.\nContact: Philip Kufeldt; e-mail: pak@integratus.com\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"05bf00d67c7af7cb6fabe6ac88e52cc9","permalink":"https://skyhookdm.github.io/author/philip-kufeldt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/philip-kufeldt/","section":"authors","summary":"Kinetic Open Storage Project Update\nAbstract: Prior to departing Seagate I was able to open source the remaining components of Seagate’s Kinetic implementation, namely their server, kineticd. This kinetic server implementation was used inside their production 4T and 8T Kinetic HDD offerings.","tags":null,"title":"Philip Kufeldt","type":"authors"},{"authors":["qing.zheng"],"categories":null,"content":"C2: LANL-Seagate’s Early Prototype for Near-Data, SQL-Like Query Processing\nAbstract: High-performance computing data centers supporting large-scale simulation applications can routinely generate a tremendous amount of data. For cost-effective high bandwidth, many data centers have used tiered storage with warmer tiers made of flashes and cooler tiers provisioned with high-density rotational disk drives. While recent advances in storage media, high-speed interconnects, and systems software have greatly improved modern platforms’ ability to handle massive data, highly selective data retrievals, such as those in the form of SQL-like queries with predicates on potentially multiple data columns, tend to still experience excessive delays when unordered, unindexed data written in log-structured formats for high write bandwidth is subsequently read for interactive data analytics. Queries run slowly because an entire dataset may have to be transferred from storage to worker nodes, even when only a small portion is actually relevant. In the worst case, significant delays are experienced even when data is read from warm storage. A user sees even higher delays when data must be streamed from cool storage tiers.\nIn this presentation, we present C2, a research collaboration between Seagate and Los Alamos National Lab (LANL) for the lab’s next-generation campaign storage. Campaign is a scalable cool storage tier at LANL managed by MarFS that currently provides 100 PBs of storage space for long-term data storage. Cost-effective data protection is done through multi-level erasure coding at both node level and rack level. To prevent users from potentially having to read back an entire dataset, C2 enables direct SQL-like query processing at the storage level by leveraging Seagate Kinetic Computational Storage Drives for near data processing. Combining computational storage technologies with erasure coding based data protection schemes for rapid data analytics over cool storage presents unique challenges in which individual disk drives may not be able to see complete data records and may not deliver performance required by high-level data applications. We discuss those challenges in the talk, share our designs, and report early results. Our results show that computational storage shows higher performance when host-storage interconnection bandwidth is a bottleneck, when host CPU is a bottleneck, and, perhaps surprisingly, when hosts do not have any obvious bottlenecks.\nBio: Qing Zheng is a Scientist in Los Alamos National Lab’s High-Performance Computing Division and a member of the Lab’s Ultrascale System Research Center. Qing performs I/O and storage research that guides the Lab’s future computing platform and storage infrastructure designs. Qing received his PhD in Computer Science from Carnegie Mellon University in 2021. Qing is known for his expertise in distributed filesystem metadata and large-scale data analytics. Qing’s work has been exhibited at local science museums, reported by national media, and recognized with multiple R\u0026amp;D 100 and Supercomputing Best Paper Awards.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"63d108293abdec9a69d70d278325035e","permalink":"https://skyhookdm.github.io/author/qing-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qing-zheng/","section":"authors","summary":"C2: LANL-Seagate’s Early Prototype for Near-Data, SQL-Like Query Processing\nAbstract: High-performance computing data centers supporting large-scale simulation applications can routinely generate a tremendous amount of data. For cost-effective high bandwidth, many data centers have used tiered storage with warmer tiers made of flashes and cooler tiers provisioned with high-density rotational disk drives.","tags":null,"title":"Qing Zheng","type":"authors"},{"authors":["slieggi"],"categories":null,"content":"Stephanie Lieggi has been Assistant Director of CROSS since 2016. Prior to coming to UCSC, she worked as a Senior Research Associate at the Center for Nonproliferation Studies at the Middlebury Institute of International Studies at Monterey. She has served as an editor of the CNS publications Asian Export Control Observer and International Export Control Observer. Previously, she worked at the Organization for the Prohibition of Chemical Weapons.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9823a88490ceee683395454ad1589a0f","permalink":"https://skyhookdm.github.io/author/stephanie-lieggi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/stephanie-lieggi/","section":"authors","summary":"Stephanie Lieggi has been Assistant Director of CROSS since 2016. Prior to coming to UCSC, she worked as a Senior Research Associate at the Center for Nonproliferation Studies at the Middlebury Institute of International Studies at Monterey.","tags":null,"title":"Stephanie Lieggi","type":"authors"},{"authors":["yoichiro.tanaka"],"categories":null,"content":"Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration\nAbstract: The digital world is producing nearly a hundred Zetta bytes of data per year and creating values for the quality of society. A huge amount of data is being stored, processed, transmitted, and then shared via large scale networked datacenters which consist of millions of data storage systems filled with perpendicular magnetic recording (PMR) hard disk drives. The PMR technology was invented by Shunichi Iwasaki in 1975 and the first commercial product was launched in 2005. Since then, the data storage has become the ever-growing foundation of the digital world and led the data-driven innovations such as bigdata AI analytics, internet of things, medical science, and even a blackhole visualization in astronomy.\nThis lecture will provide the essential magnetics to create innovative data storage technology of PMR and the origin of the high-density recording performance which has led current recording density growth. The storage performance stands on the stacked system foundation and the building blocks are, from the base, physics of magnetics, 3D material controls of sub-nanometer in size, magnetic and electronic device design, storage device integration, and storage system architectures together with non-volatile memories to unleash the intrinsic performance. The development of new storage devices and the system requires a multi-scale approach and a right guiding principle to establish expected functions. As an extension of PMR research, the lecture will also show the prospect of future storage technology and the system architecture from the multi-scale view of the storage system development. A new computational storage system aiming at unifying computation power on data store and brain-inspired system considerations as well as the academism-industry relations to realize those systems will also be introduced.\nBio: Yoichiro Tanaka is a professor in Research Institute of Electrical Communication (RIEC) at Tohoku University. He received the Bachelor, the Master degrees in Communications Engineering and the Ph.D degree in Electronic Engineering from Tohoku University, where he did his thesis research on perpendicular megnetic recording physics and the storage system integrations. He had worked in Toshiba’s storage device business and in academia over 30 years. He devoted to the proof and development of PMR and achieved the realization of the world’s first PMR HDD in 2005. His carrier includes the development of a giant magnetoresistive head, the perpendicular glanular thin film media, and recently a new computational storage systems with integrated bigdata analytics capability. He won the Nikkei BP Technology Awards (1997, 2006), the Japan Magnetic Society Achievement Award (2006) and Okochi Memorial Prize (2007). He is a senior member of IEEE and a member of Magnetic Society Administrative Committee for a three-year term 2022-2024. He is also a fellow of the Magnetic Society of Japan (MSJ). He is currently serving as the Secretary General for Intermatinal Magnetics Conference (INTERMAG 2023 Sendai).\nContact: Yoichiro Tanaka, Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, Japan 980-8577; e-mail: yoichiro.tanaka.e1@tohoku.ac.jp\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e3cae27a7e236132aa7721100f1b62dd","permalink":"https://skyhookdm.github.io/author/yoichiro-tanaka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yoichiro-tanaka/","section":"authors","summary":"Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration\nAbstract: The digital world is producing nearly a hundred Zetta bytes of data per year and creating values for the quality of society.","tags":null,"title":"Yoichiro Tanaka","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://skyhookdm.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":null,"content":"👋 Join us for an exciting event featuring IEEE Distinguished Lecturer Yoichiro Tanaka (Tohoku University) discussing technological and institutional innovations to make the computational I/O stack a reality!\nRegister –\u0026gt; The event is hybrid – we highly encourage in-person attendance and welcome remote participation. The introduction of computational data management services into the I/O stack, especially in storage and networking devices, requires both technological innovations and new relations between university and industry. This one-day workshop will convene experts from storage systems, open source, and community architecture to discuss technologies and strategies for a computational I/O stack with low market entry barriers.\nThe workshop will take place on August 17, 2023 from 10am to 5pm, at UC Santa Cruz, Engineering 2 (room to be confirmed) and is jointly organized by the IEEE Magnetics Society’s Distinguished Lecturers Program, the Skyhook Data Management community with funding by the National Science Foundation (TI-2229773), the Center for Research in Open Source Software (cross.ucsc.edu), and the Open Source Program Office, UC Santa Cruz (ospo.ucsc.edu).\nRegister –\u0026gt; Time Title Speaker(s) 10-10:15am Introduction Carlos Maltzahn (UC Santa Cruz), Philip Kufeldt 10:15-noon Session 1 10:15-11:15am IEEE Distinguished Lecture: Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration Yoichiro Tanaka (Tohoku University) 11:15-11:30am Q\u0026amp;A and Discussion 11:30-11:45am Institutional Support for Experimental Systems Research Carlos Maltzahn (UC Santa Cruz) 11:45-noon Q\u0026amp;A and Discussion noon-1pm Lunch (provided) 1-2:35pm Session 2 1-1:20pm Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface Pankaj Mehra (UCSC, Elephance Memory) 1:20-1:35pm TBD TBD (Voltron Data) 1:35-1:50pm Kinetic Open Storage Update Philip Kufeldt 1:50-2:05pm Skytether Aldrin Montana (UC Santa Cruz) 2:05-2:35pm C2: LANL-Seagate’s Early Prototype for Near-Data, SQL-Like Query Processing Qing Zheng (Los Alamos National Lab) 2:35-3:05pm Break (refreshments provided) 3:05-3.55pm Session 3 3:05-3:25pm Extending Composable Data Services into SmartNICS Craig Ulmer (Sandia National Labs) 3:25-3:40pm Opportunistic Querying in SmartNICs Jianshen Liu (UC Santa Cruz) 3:40-3:55pm An HPC-Oriented Runtime Environment for Enabling Computational Storage Matthew L. Curry (Sandia National Labs) 3:55-4:25pm Break (refreshments provided) 4:25-5pm Session 4 4:25-4:40pm Towards Faster Columnar Data Transport using RDMA Jayjeet Chakraborty (UC Santa Cruz) 4:40-4:50 CSAssist: A Methodology to Assist Kernel Offloading Decisions on Computational Storage Lokesh Jaliminche (UC Santa Cruz) 4:50-5pm RPC offloading to SmartNICs Esteban Ramos (UC Santa Cruz) Table: Preliminary Agenda Register –\u0026gt; ","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552e3,"objectID":"039125c9423a97146763e71999b88f58","permalink":"https://skyhookdm.github.io/post/20230718-aug17/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/post/20230718-aug17/","section":"post","summary":"A workshop featuring keynote speaker Yoichiro Tanaka (Tohoku University) that will take place from 10am to 3pm (PT), at UC Santa Cruz in the Engineering 2 building (room to be confirmed).","tags":null,"title":"Computational I/O Stack Workshop, August 17, 2023","type":"post"},{"authors":["Craig Ulmer","Jianshen Liu","Carlos Maltzahn","Matthew L. Curry"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"2c118178960eeba9b4a0adab34e2539b","permalink":"https://skyhookdm.github.io/publication/ulmer-compsys-23/","publishdate":"2023-06-08T01:20:50.634899Z","relpermalink":"/publication/ulmer-compsys-23/","section":"publication","summary":"Advanced scientific-computing workflows rely on composable data services to migrate data between simulation and analysis jobs that run in parallel on high-performance computing (HPC) platforms. Unfortunately, these services consume compute-node memory and processing resources that could otherwise be used to complete the workflow’s tasks. The emergence of programmable network interface cards, or SmartNICs, presents an opportunity to host data services in an isolated space within a compute node that does not impact host resources. In this paper we explore extending data services into SmartNICs and describe a software stack for services that uses Faodel and Apache Arrow. To illustrate how this stack operates, we present a case study that implements a distributed, particle-sifting service for reorganizing simulation results. Performance experiments from a 100-node cluster equipped with 100Gb/s BlueField-2 SmartNICs indicate that current SmartNICs can perform useful data management tasks, albeit at a lower throughput than hosts.","tags":["smartnics","composability","datamanagement"],"title":"Extending Composable Data Services into SmartNICS (Best Paper Award)","type":"publication"},{"authors":["Jianshen Liu","Carlos Maltzahn","Matthew L. Curry","Craig Ulmer"],"categories":[],"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660699385,"objectID":"97962b126e6f272e3726162f17fd8f88","permalink":"https://skyhookdm.github.io/publication/liu-hpec-22/","publishdate":"2022-08-17T01:23:04.962887Z","relpermalink":"/publication/liu-hpec-22/","section":"publication","summary":"Many distributed applications implement complex data flows and need a flexible mechanism for routing data between producers and consumers. Recent advances in programmable network interface cards, or SmartNICs, represent an opportunity to offload data-flow tasks into the network fabric, thereby freeing the hosts to perform other work. System architects in this space face multiple questions about the best way to leverage SmartNICs as processing elements in data flows. In this paper, we advocate the use of Apache Arrow as a foundation to implement data flow tasks on SmartNICs. We report on our experience adapting a partitioning algorithm for particle data to Apache Arrow and measure the on-card processing performance for the BlueField-2 SmartNIC. Our experiments confirm that the BlueField-2's (de)compression hardware can have a significant impact on in-transit workflows where data must be unpacked, processed, and repacked.","tags":["smartnics","offloading","datamanagement","hpc"],"title":"Processing Particle Data Flows with SmartNICs (Outstanding Student Paper)","type":"publication"},{"authors":["Jayjeet Chakraborty","Ivo Jimenez","Sebastiaan Alvarez Rodriguez","Alexandru Uta","Jeff LeFevre","Carlos Maltzahn"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650924479,"objectID":"c94e7367e6c8ebf718d751375b36023b","permalink":"https://skyhookdm.github.io/publication/chakraborty-ccgrid-22/","publishdate":"2022-04-25T22:07:44.206228Z","relpermalink":"/publication/chakraborty-ccgrid-22/","section":"publication","summary":"With the ever-increasing dataset sizes, several file formats such as Parquet, ORC, and Avro have been developed to store data efficiently, save the network, and interconnect bandwidth at the price of additional CPU utilization. However, with the advent of networks supporting 25-100 Gb/s and storage devices delivering 1,000,000 reqs/sec, the CPU has become the bottleneck trying to keep up feeding data in and out of these fast devices. The result is that data access libraries executed on single clients are often CPU-bound and cannot utilize the scale-out benefits of distributed storage systems. One attractive solution to this problem is to offload data-reducing processing and filtering tasks to the storage layer. However, modifying legacy storage systems to support compute offloading is often tedious and requires an extensive understanding of the system internals. Previous approaches re-implemented functionality of data processing frameworks and access libraries for a particular storage system, a duplication of effort that might have to be repeated for different storage systems.  This paper introduces a new design paradigm that allows extending programmable object storage systems to embed existing, widely used data processing frameworks and access libraries into the storage layer with no modifications. In this approach, data processing frameworks and access libraries can evolve independently from storage systems while leveraging distributed storage systems' scale-out and availability properties. We present Skyhook, an example implementation of our design paradigm using Ceph, Apache Arrow, and Parquet. We provide a brief performance evaluation of Skyhook and discuss key results.","tags":["papers","programmable","storage","systems","arrow","nsf1836650","nsf1705021","nsf1764102"],"title":"Skyhook: Towards an Arrow-Native Storage System","type":"publication"},{"authors":["Jayjeet Chakraborty","Carlos Maltzahn","David Li","Tom Drabas"],"categories":[],"content":"","date":1643587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688515200,"objectID":"c05a2dfe467c799ab0607b586ff94793","permalink":"https://skyhookdm.github.io/publication/chakraborty-arrowblog-22/","publishdate":"2022-05-08T18:22:08.329823Z","relpermalink":"/publication/chakraborty-arrowblog-22/","section":"publication","summary":"CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they’re improving in different dimensions. Processors are faster, but their memory bandwidth hasn’t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link. This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\nFor example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client. Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter. While formats like Apache Parquet enable some optimizations, fundamentally, the responsibility is all on the client. Meanwhile, even though the storage system has its own compute capabilities, it’s relegated to just serving “dumb bytes”.\nThanks to the [Center for Research in Open Source Software](https://cross.ucsc.edu) (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets](https://arrow.apache.org/docs/cpp/dataset.html) extension that solves this problem by using the storage layer to reduce client resource utilization. We’ll examine the developments surrounding Skyhook as well as how Skyhook works.","tags":["computation","storage","programmable","datamanagement","ceph","arrow"],"title":" Skyhook: Bringing Computation to Storage with Apache Arrow ","type":"publication"},{"authors":["Sebastiaan Alvarez Rodriguez","Jayjeet Chakraborty","Aaron Chu","Ivo Jimenez","Jeff LeFevre","Carlos Maltzahn","Alexandru Uta"],"categories":[],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650924479,"objectID":"efc006628dfcfb689d00a92ffbcf4939","permalink":"https://skyhookdm.github.io/publication/rodriguez-bigdata-21/","publishdate":"2022-04-25T22:07:59.443656Z","relpermalink":"/publication/rodriguez-bigdata-21/","section":"publication","summary":"Distributed data processing ecosystems are widespread and their components are highly specialized, such that efficient interoperability is urgent. Recently, Apache Arrow was chosen by the community to serve as a format mediator, providing efficient in-memory data representation. Arrow enables efficient data movement between data processing and storage engines, significantly improving interoperability and overall performance. In this work, we design a new zero-cost data interoperability layer between Apache Spark and Arrow-based data sources through the Arrow Dataset API. Our novel data interface helps separate the computation (Spark) and data (Arrow) layers. This enables practitioners to seamlessly use Spark to access data from all Arrow Dataset API-enabled data sources and frameworks. To benefit our community, we open-source our work and show that consuming data through Apache Arrow is zero-cost: our novel data interface is either on-par or more performant than native Spark.","tags":["papers","spark","arrow","performance","nsf1836650"],"title":"Zero-Cost, Arrow-Enabled Data Interface for Apache Spark","type":"publication"},{"authors":["Aaron Chu","Jeff LeFevre","Carlos Maltzahn","Aldrin Montana","Peter Alvaro","Dana Robinson","Quincey Koziol"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"fea189160dee93ce7424cc48371e9981","permalink":"https://skyhookdm.github.io/publication/chu-epjconf-20/","publishdate":"2021-02-21T00:24:01.226701Z","relpermalink":"/publication/chu-epjconf-20/","section":"publication","summary":"Access libraries such as ROOT and HDF5 allow users to interact with datasets using high level abstractions, like coordinate systems and associated slicing operations. Unfortunately, the implementations of access libraries are based on outdated assumptions about storage systems interfaces and are generally unable to fully benefit from modern fast storage devices. For example, access libraries often implement buffering and data layout that assume that large, single-threaded sequential access patterns are causing less overall latency than small parallel random access: while this is true for spinning media, it is not true for flash media. The situation is getting worse with rapidly evolving storage devices such as non-volatile memory and ever larger datasets. Our Skyhook Dataset Mapping project explores distributed dataset mapping infrastructures that can integrate and scale out existing access libraries using Ceph's extensible object model, avoiding reimplementation or even modifications of these access libraries as much as possible. These programmable storage extensions coupled with our distributed dataset mapping techniques enable: 1) access library operations to be offloaded to storage system servers, 2) the independent evolution of access libraries and storage systems and 3) fully leveraging of the existing load balancing, elasticity, and failure management of distributed storage systems like Ceph. They also create more opportunities to conduct storage server-local optimizations specific to storage servers. For example, storage servers might include local key/value stores combined with chunk stores that require different optimizations than a local file system. As storage servers evolve to support new storage devices like non-volatile memory, these server-local optimizations can be implemented while minimizing disruptions to applications. We will report progress on the means by which distributed dataset mapping can be abstracted over particular access libraries, including access libraries for ROOT data, and how we address some of the challenges revolving around data partitioning and composability of access operations.","tags":["papers","programmable","declarative","objectstorage","nsf1836650"],"title":"Mapping Scientific Datasets to Programmable Storage","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"865e411be877820cdabae9f09315eb30","permalink":"https://skyhookdm.github.io/publication/lefevre-snia-20/","publishdate":"2023-01-26T14:23:16.862545Z","relpermalink":"/publication/lefevre-snia-20/","section":"publication","summary":"","tags":["programmable","storage"],"title":"SkyhookDM: Storage and Management of Tabular Data in Ceph","type":"publication"},{"authors":["Jianshen Liu","Matthew Leon Curry","Carlos Maltzahn","Philip Kufeldt"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f7340b531adf3e2f68f81ac006cc8e75","permalink":"https://skyhookdm.github.io/publication/liu-hotedge-20/","publishdate":"2020-04-19T19:42:33.148866Z","relpermalink":"/publication/liu-hotedge-20/","section":"publication","summary":"In the resource-rich environment of data centers most failures can quickly failover to redundant resources. In contrast, failure in edge infrastructures with limited resources might require maintenance personnel to drive to the location in order to fix the problem. The operational cost of these 'truck rolls' to locations at the edge infrastructure competes with the operational cost incurred by extra space and power needed for redundant resources at the edge. Computational storage devices with network interfaces can act as network-attached storage servers and offer a new design point for storage systems at the edge. In this paper we hypothesize that a system consisting of a larger number of such small 'embedded' storage nodes provides higher availability due to a larger number of failure domains while also saving operational cost in terms of space and power. As evidence for our hypothesis, we compared the possibility of data loss between two different types of storage systems: one is constructed with general-purpose servers, and the other one is constructed with embedded storage nodes. Our results show that the storage system constructed with general-purpose servers has 7 to 20 times higher risk of losing data over the storage system constructed with embedded storage devices. We also compare the two alternatives in terms of power and space using the Media-Based Working Unit (MBWU) that we developed in an earlier paper as a reference point.","tags":["papers","edge","reliability","disaggregation","embedded","failures"],"title":"Scale-out Edge Storage Systems with Embedded Storage Nodes to Get Better Availability and Cost-Efficiency At the Same Time","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":159192e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":159192e4,"objectID":"915e552d840309dc5886f66457e45fc1","permalink":"https://skyhookdm.github.io/publication/lefevre-login-20/","publishdate":"2020-06-13T01:39:15.018368Z","relpermalink":"/publication/lefevre-login-20/","section":"publication","summary":"","tags":["papers","programmable","storage","ceph","physicaldesign"],"title":"SkyhookDM: Data Processing in Ceph with Programmable Storage","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"c0997a47ffd719cd2ea0c0d4f82ae9a9","permalink":"https://skyhookdm.github.io/publication/lefevre-vault-20/","publishdate":"2020-01-05T06:43:50.391599Z","relpermalink":"/publication/lefevre-vault-20/","section":"publication","summary":"","tags":["shortpapers","programmable","storage","physicaldesign"],"title":"Scaling databases and file APIs with programmable Ceph object storage","type":"publication"},{"authors":["Kathryn Dahlgren","Jeff LeFevre","Ashay Shirwadkar","Ken Iizawa","Aldrin Montana","Peter Alvaro","Carlos Maltzahn"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"e615944dd1d3f72bced85e81bb5a0490","permalink":"https://skyhookdm.github.io/publication/dahlgren-pdsw-19/","publishdate":"2020-01-05T06:43:50.411373Z","relpermalink":"/publication/dahlgren-pdsw-19/","section":"publication","summary":"In the post-Moore era, systems and devices with new architectures will arrive at a rapid rate with significant impacts on the software stack. Applications will not be able to fully benefit from new architectures unless they can delegate adapting to new devices in lower layers of the stack. In this paper we introduce physical design management which deals with the problem of identifying and executing transformations on physical designs of stored data, i.e. how data is mapped to storage abstractions like files, objects, or blocks, in order to improve performance. Physical design is traditionally placed with applications, access libraries, and databases, using hard- wired assumptions about underlying storage systems. Yet, storage systems increasingly not only contain multiple kinds of storage devices with vastly different performance profiles but also move data among those storage devices, thereby changing the benefit of a particular physical design. We advocate placing physical design management in storage, identify interesting research challenges, provide a brief description of a prototype implementation in Ceph, and discuss the results of initial experiments at scale that are replicable using Cloudlab. These experiments show performance and resource utilization trade-offs associated with choosing different physical designs and choosing to transform between physical designs.","tags":["papers","programmable","storage","datamanagement","physicaldesign"],"title":"Towards Physical Design Management in Storage Systems","type":"publication"},{"authors":["Jeff LeFevre","Noah Watkins","Michael Sevilla","Carlos Maltzahn"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"a06d72e4b291d58378ac21f0662aef77","permalink":"https://skyhookdm.github.io/publication/lefevre-vault-19/","publishdate":"2020-01-05T06:43:50.416084Z","relpermalink":"/publication/lefevre-vault-19/","section":"publication","summary":"Ceph is an open source distributed storage system that is object-based and massively scalable. Ceph provides developers with the capability to create data interfaces that can take advantage of local CPU and memory on the storage nodes (Ceph Object Storage Devices). These interfaces are powerful for application developers and can be created in C, C++, and Lua.\nSkyhook is an open source storage and database project in the Center for Research in Open Source Software at UC Santa Cruz. Skyhook uses these capabilities in Ceph to create specialized read/write interfaces that leverage IO and CPU within the storage layer toward database processing and management. Specifically, we develop methods to apply predicates locally as well as additional metadata and indexing capabilities using Ceph's internal indexing mechanism built on top of RocksDB.\nSkyhook's approach helps to enable scale-out of a single node database system by scaling out the storage layer. Our results show the performance benefits for some queries indeed scale well as the storage layer scales out.","tags":["papers","programmable","storage","database"],"title":"Skyhook: Programmable storage for databases","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"00a590b7b829ae9eaf53b0eb511e8800","permalink":"https://skyhookdm.github.io/cft/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cft/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://skyhookdm.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://skyhookdm.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":" Under construction – Will be completed soon. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"690765627f118164601e27255229899f","permalink":"https://skyhookdm.github.io/conduct/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/conduct/","section":"","summary":" Under construction – Will be completed soon. ","tags":null,"title":"Code of Conduct","type":"page"},{"authors":null,"categories":null,"content":" Under construction – Will be completed soon. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a4ba8b5ca1d988c2465a7d545ab37f3","permalink":"https://skyhookdm.github.io/diversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/diversity/","section":"","summary":" Under construction – Will be completed soon. ","tags":null,"title":"Diversity \u0026 Inclusion","type":"page"},{"authors":null,"categories":null,"content":" Under construction – Will be completed soon. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae2b9e20728a7935c40f33b3013854dd","permalink":"https://skyhookdm.github.io/keynotes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/keynotes/","section":"","summary":" Under construction – Will be completed soon. ","tags":null,"title":"Featured Keynote Speakers","type":"page"},{"authors":null,"categories":null,"content":" Under construction – Will be completed soon. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a037c91e31885d40e3515179cea37ff2","permalink":"https://skyhookdm.github.io/health/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/health/","section":"","summary":" Under construction – Will be completed soon. ","tags":null,"title":"Health \u0026 Safety","type":"page"}]