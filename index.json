[{"authors":["aldrin.montana"],"categories":null,"content":"Aldrin is a PhD candidate at the University of California, Santa Cruz. He is advised by Peter Alvaro and also works with Josh Stuart, Jeff LeFevre, and Carlos Maltzahn. His research interests primarily include bioinformatics, data management systems, and storage systems. His current research focuses on distributing query plans across computational devices in a distributed system where the devices are organized hierarchically and may have a wide variety of device characteristics.\n","date":1692144e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1692144e3,"objectID":"ef1ff1397a23e07714e85c783270915e","permalink":"https://skyhookdm.github.io/author/aldrin-montana/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aldrin-montana/","section":"authors","summary":"Aldrin is a PhD candidate at the University of California, Santa Cruz. He is advised by Peter Alvaro and also works with Josh Stuart, Jeff LeFevre, and Carlos Maltzahn. His research interests primarily include bioinformatics, data management systems, and storage systems.","tags":null,"title":"Aldrin Montana","type":"authors"},{"authors":["carlos.maltzahn"],"categories":null,"content":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026amp; processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems. Carlos joined UC Santa Cruz in 2004, after five years at Netapp working on network-intermediaries and storage systems. In 2005 he co-founded and became a key mentor on Sage Weil‚Äôs Ceph project. In 2008 Carlos became a member of the computer science faculty at UC Santa Cruz and has graduated nine Ph.D. students since. Carlos graduated with a M.S. and Ph.D. in Computer Science from University of Colorado at Boulder.\nHis work is funded by nonprofits, government, and industry, including NSF OAC-2226407, the Alfred P. Sloan Foundation G-2021-16957, DOE ASCR DE-NA0003525 (FWP 20-023266, subcontractor of Sandia National Labs), NSF OAC-1836650, NSF CNS-1764102, NSF CNS-1705021, DOE ASCR DE-SC0016074, NSF OAC-1450488, and CROSS\nFor more details, see his homepage and his vitae.\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"4fb4e5f354a0383d4343506601348c60","permalink":"https://skyhookdm.github.io/author/carlos-maltzahn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/carlos-maltzahn/","section":"authors","summary":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026 processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems.","tags":null,"title":"Carlos Maltzahn","type":"authors"},{"authors":["craigulmer"],"categories":null,"content":"Craig Ulmer is a Principal Member of Technical Staff in the Computation \u0026amp; Analysis for National Security Center at Sandia National Laboratories in Livermore, California. His research focuses on adapting new hardware and software technologies to solve data-intensive problems more efficiently. During his 20 years of service at Sandia, he has worked with the scientific computing, nonproliferation, and cybersecurity mission spaces and led efforts to integrate SmartNICs, GPUs, NVMe, and FPGAs into production workflows. Prior to joining Sandia, Craig received a Ph.D. in Electrical and Computer Engineering from the Georgia Institute of Technology for his work in low-level communication software for resource-rich cluster computers.\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"44f0e23b02c2bb9533ff0f544e85b865","permalink":"https://skyhookdm.github.io/author/craig-ulmer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/craig-ulmer/","section":"authors","summary":"Craig Ulmer is a Principal Member of Technical Staff in the Computation \u0026 Analysis for National Security Center at Sandia National Laboratories in Livermore, California. His research focuses on adapting new hardware and software technologies to solve data-intensive problems more efficiently.","tags":null,"title":"Craig Ulmer","type":"authors"},{"authors":["esteban.ramos"],"categories":null,"content":"Esteban Ramos is a PhD student at the University of California, Santa Cruz. He is advised by Dr. Peter Alvaro. He is interested in distributed systems, data management systems, and networking. He is currently working on projects that seek to minimize overhead introduced by RPCs in microservice applications.\nContact: Esteban Ramos; e-mail: esiramos@ucsc.edu\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"b090134a26ec044e88798734e28486a6","permalink":"https://skyhookdm.github.io/author/esteban-ramos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/esteban-ramos/","section":"authors","summary":"Esteban Ramos is a PhD student at the University of California, Santa Cruz. He is advised by Dr. Peter Alvaro. He is interested in distributed systems, data management systems, and networking.","tags":null,"title":"Esteban Ramos","type":"authors"},{"authors":["jayjeet.chakraborty"],"categories":null,"content":"Jayjeet Chakraborty is a PhD student at the University of California, Santa Cruz. He is advised by Carlos Maltzahn. His research interests include Data management and Database systems, Computational storage systems, Distributed systems, and Systems in general. For his PhD, he is working with Argonne National Labs on building a hardware-accelerated columnar data transport frameworks leveraging RDMA.\nContact: Jayjeet Chakraborty; e-mail: jayjeetc@ucsc.edu\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"5b97a8deeea47982adec66d16c00b787","permalink":"https://skyhookdm.github.io/author/jayjeet-chakraborty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jayjeet-chakraborty/","section":"authors","summary":"Jayjeet Chakraborty is a PhD student at the University of California, Santa Cruz. He is advised by Carlos Maltzahn. His research interests include Data management and Database systems, Computational storage systems, Distributed systems, and Systems in general.","tags":null,"title":"Jayjeet Chakraborty","type":"authors"},{"authors":["jianshen.liu"],"categories":null,"content":"Jianshen Liu recently earned his Ph.D. in Computer Science from University of California, Santa Cruz, under the supervision of Prof. Carlos Maltzahn. As a graduate student researcher at the university, he focused on exploring the opportunities and benefits of utilizing computational storage devices and SmartNICs, developing methods and tools to harness their potential. He interned at Samsung and Kioxia, focusing on computational storage research. He is currently starting a career as a software engineer with a globally recognized information and communications technology infrastructure leader.\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"a557d342b408466610717a7a70fd1f4c","permalink":"https://skyhookdm.github.io/author/jianshen-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianshen-liu/","section":"authors","summary":"Jianshen Liu recently earned his Ph.D. in Computer Science from University of California, Santa Cruz, under the supervision of Prof. Carlos Maltzahn. As a graduate student researcher at the university, he focused on exploring the opportunities and benefits of utilizing computational storage devices and SmartNICs, developing methods and tools to harness their potential.","tags":null,"title":"Jianshen Liu","type":"authors"},{"authors":["lokesh.jaliminche"],"categories":null,"content":"Lokesh Jaliminche is a Ph.D. candidate at the University of California, Santa Cruz. He is advised by Prof. Heiner Litz. His research interests revolve around Storage Systems, Distributed Systems, applied Machine learning to improve QoS, and Computational storage. Currently, he is investigating a methodology to assist kernel offloading decisions on Computational Storage.\nContact: Lokesh Jaliminche; e-mail: ljalimin@ucsc.edu\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"655d65dd121bce530fc939290a790a5e","permalink":"https://skyhookdm.github.io/author/lokesh-jaliminche/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lokesh-jaliminche/","section":"authors","summary":"Lokesh Jaliminche is a Ph.D. candidate at the University of California, Santa Cruz. He is advised by Prof. Heiner Litz. His research interests revolve around Storage Systems, Distributed Systems, applied Machine learning to improve QoS, and Computational storage.","tags":null,"title":"Lokesh Jaliminche","type":"authors"},{"authors":["matthew.curry"],"categories":null,"content":"Matthew Curry has a wide variety of research interests, mostly pertaining directly to data storage in supercomputing environments, including parallel and distributed storage systems, erasure coding and fault tolerance, low-level (block or object) storage devices, and heterogeneous computing (e.g., GPUs, accelerators). He graduated with a Ph.D. in Computer Science at University of Alabama at Birmingham under the advisement of Dr. Anthony Skjellum.\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"d5ba6a3324f6e6276c80c4e4342b24ff","permalink":"https://skyhookdm.github.io/author/matthew-curry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthew-curry/","section":"authors","summary":"Matthew Curry has a wide variety of research interests, mostly pertaining directly to data storage in supercomputing environments, including parallel and distributed storage systems, erasure coding and fault tolerance, low-level (block or object) storage devices, and heterogeneous computing (e.","tags":null,"title":"Matthew Curry","type":"authors"},{"authors":["pankaj.mehra"],"categories":null,"content":"Pankaj Mehra is President and CEO of Elephance Memory. A contributor to InfiniBand 1.0 spec and an inventor of the first RDMA persistent memory devices and filesystems, Pankaj is a systems and software practitioner who conducts his research at the intersection of infrastructure and intelligence. He is a co-author/co-editor/co-inventor on 3 books, and over 100 papers and patents, and a builder of systems that have won recognition from NASA, Sandia National Labs, and Samsung Electronics, among others. He was a VP of Storage Pathfinding at Samsung, Senior Fellow and VP at SanDisk and Western Digital, WW CTO of Fusion-io, and Distinguished Technologist at HP Labs. Pankaj has held faculty positions at IIT Delhi and UC Santa Cruz.\n","date":1691712e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1691712e3,"objectID":"ac06de0c8dee491d8b860b0acc0cb6e3","permalink":"https://skyhookdm.github.io/author/pankaj-mehra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pankaj-mehra/","section":"authors","summary":"Pankaj Mehra is President and CEO of Elephance Memory. A contributor to InfiniBand 1.0 spec and an inventor of the first RDMA persistent memory devices and filesystems, Pankaj is a systems and software practitioner who conducts his research at the intersection of infrastructure and intelligence.","tags":null,"title":"Pankaj Mehra","type":"authors"},{"authors":["philip.kufeldt"],"categories":null,"content":"Philip Kufeldt‚Äôs last role was a Technologist in Seagate Research. He focuses on computational storage using smart HDDs. He has 30 years of experience in storage, software, and systems. Before Seagate, Philip was at Toshiba, creating new smart media devices and founding the Kinetic Open Storage Project; additionally he has spent time at UCSC, Marvell, Parascale, VERITAS Software, Sun Microsystems, and IBM; and he has founded several storage oriented startups.\nContact: Philip Kufeldt; e-mail: pak@integratus.com\n","date":1689552e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689552e3,"objectID":"05bf00d67c7af7cb6fabe6ac88e52cc9","permalink":"https://skyhookdm.github.io/author/philip-kufeldt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/philip-kufeldt/","section":"authors","summary":"Philip Kufeldt‚Äôs last role was a Technologist in Seagate Research. He focuses on computational storage using smart HDDs. He has 30 years of experience in storage, software, and systems. Before Seagate, Philip was at Toshiba, creating new smart media devices and founding the Kinetic Open Storage Project; additionally he has spent time at UCSC, Marvell, Parascale, VERITAS Software, Sun Microsystems, and IBM; and he has founded several storage oriented startups.","tags":null,"title":"Philip Kufeldt","type":"authors"},{"authors":["qing.zheng"],"categories":null,"content":"Qing Zheng is a Scientist in Los Alamos National Lab‚Äôs High-Performance Computing Division and a member of the Lab‚Äôs Ultrascale System Research Center. Qing performs I/O and storage research that guides the Lab‚Äôs future computing platform and storage infrastructure designs. Qing received his PhD in Computer Science from Carnegie Mellon University in 2021. Qing is known for his expertise in distributed filesystem metadata and large-scale data analytics. Qing‚Äôs work has been exhibited at local science museums, reported by national media, and recognized with multiple R\u0026amp;D 100 and Supercomputing Best Paper Awards.\n","date":1689552e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689552e3,"objectID":"63d108293abdec9a69d70d278325035e","permalink":"https://skyhookdm.github.io/author/qing-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qing-zheng/","section":"authors","summary":"Qing Zheng is a Scientist in Los Alamos National Lab‚Äôs High-Performance Computing Division and a member of the Lab‚Äôs Ultrascale System Research Center. Qing performs I/O and storage research that guides the Lab‚Äôs future computing platform and storage infrastructure designs.","tags":null,"title":"Qing Zheng","type":"authors"},{"authors":["yoichiro.tanaka"],"categories":null,"content":"Yoichiro Tanaka is a professor at the Research Institute of Electrical Communication (RIEC) at Tohoku University. He received Bachelor and Master degrees in Communications Engineering and a Ph.D degree in Electronic Engineering from Tohoku University, where he did his thesis research on perpendicular magnetic recording physics and storage systems integrations. He worked in Toshiba‚Äôs storage device business and in academia over 30 years. He was devoted to the validation and development of PMR and achieved the realization of the world‚Äôs first PMR HDD in 2005. His career includes the development of giant magnetoresistive heads, perpendicular granular thin film media, and recently a new computational storage system with integrated big data analytics capability. He won the Nikkei BP Technology Awards (1997, 2006), the Magnetics Society of Japan Achievement Award (2006) and the Okochi Memorial Prize (2007). He is a senior member of IEEE and a member of the Magnetics Society Administrative Committee for a three-year term 2022-2024. He is also a fellow of the Magnetics Society of Japan (MSJ). He is currently serving as the Secretary General for the Internatinal Magnetics Conference (INTERMAG 2023 Sendai).\nContact: Yoichiro Tanaka, Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, Japan 980-8577; e-mail: yoichiro.tanaka.e1@tohoku.ac.jp\n","date":1689552e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689552e3,"objectID":"e3cae27a7e236132aa7721100f1b62dd","permalink":"https://skyhookdm.github.io/author/yoichiro-tanaka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yoichiro-tanaka/","section":"authors","summary":"Yoichiro Tanaka is a professor at the Research Institute of Electrical Communication (RIEC) at Tohoku University. He received Bachelor and Master degrees in Communications Engineering and a Ph.D degree in Electronic Engineering from Tohoku University, where he did his thesis research on perpendicular magnetic recording physics and storage systems integrations.","tags":null,"title":"Yoichiro Tanaka","type":"authors"},{"authors":["admin"],"categories":null,"content":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026amp; processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems. Carlos joined UC Santa Cruz in 2004, after five years at Netapp working on network-intermediaries and storage systems. In 2005 he co-founded and became a key mentor on Sage Weil‚Äôs Ceph project. In 2008 Carlos became a member of the computer science faculty at UC Santa Cruz and has graduated nine Ph.D. students since. Carlos graduated with a M.S. and Ph.D. in Computer Science from University of Colorado at Boulder.\nHis work is funded by nonprofits, government, and industry, including NSF OAC-2226407, the Alfred P. Sloan Foundation G-2021-16957, DOE ASCR DE-NA0003525 (FWP 20-023266, subcontractor of Sandia National Labs), NSF OAC-1836650, NSF CNS-1764102, NSF CNS-1705021, DOE ASCR DE-SC0016074, NSF OAC-1450488, and CROSS\nFor more details, see his homepage and his vitae.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://skyhookdm.github.io/author/administrator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/administrator/","section":"authors","summary":"Dr. Carlos Maltzahn is the founder and director of the UC Santa Cruz Center for Research in Open Source Software (CROSS). Dr. Maltzahn also co-founded the Systems Research Lab, known for its cutting-edge work on programmable storage systems, big data storage \u0026 processing, scalable data management, distributed system performance management, and practical reproducible evaluation of computer systems.","tags":null,"title":"Administrator","type":"authors"},{"authors":["ian.cook"],"categories":null,"content":"Product Manager at Voltron Data\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"43adb3e24003f83ae6a5bd17538d710f","permalink":"https://skyhookdm.github.io/author/ian-cook/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ian-cook/","section":"authors","summary":"Product Manager at Voltron Data","tags":null,"title":"Ian Cook","type":"authors"},{"authors":["jacques.nadeau"],"categories":null,"content":"Co-founder and CEO, Sundeck.io, co-creator of Apache Arrow, Dremio and Substrait\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f5e10adc8f0cf16c39d112bc9bb92e09","permalink":"https://skyhookdm.github.io/author/jacques-nadeau/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jacques-nadeau/","section":"authors","summary":"Co-founder and CEO, Sundeck.io, co-creator of Apache Arrow, Dremio and Substrait","tags":null,"title":"Jacques Nadeau","type":"authors"},{"authors":["slieggi"],"categories":null,"content":"Stephanie Lieggi has been Assistant Director of CROSS since 2016. Prior to coming to UCSC, she worked as a Senior Research Associate at the Center for Nonproliferation Studies at the Middlebury Institute of International Studies at Monterey. She has served as an editor of the CNS publications Asian Export Control Observer and International Export Control Observer. Previously, she worked at the Organization for the Prohibition of Chemical Weapons.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9823a88490ceee683395454ad1589a0f","permalink":"https://skyhookdm.github.io/author/stephanie-lieggi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/stephanie-lieggi/","section":"authors","summary":"Stephanie Lieggi has been Assistant Director of CROSS since 2016. Prior to coming to UCSC, she worked as a Senior Research Associate at the Center for Nonproliferation Studies at the Middlebury Institute of International Studies at Monterey.","tags":null,"title":"Stephanie Lieggi","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://skyhookdm.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Aldrin Montana"],"categories":null,"content":"Existing research in computational storage primarily explores pushdown of a single query operator or its supporting functions from a database management system. In these cases, the portion of a query that can be pushed down is static, the execution of the kernel is device-specific, and the benefits of computational storage only exist between the database system and the compuational storage devices (CS devices) it is directly connected to. However, an optimal, static partitioning is likely to change whenever workload characteristics shift, for varying device characteristics, and for evolving characteristics of the storage system.\nA storage system grows when new, heterogeneous storage devices are added to the storage hierarchy. As characteristics of these devices evolve and they gain more compute resources, it will be desirable (and necessary) to deploy heterogeneous data processing and storage engines that are well designed for device-specific characteristics or data-specific modeling and storage. I call a storage system designed for this complexity a computational storage system (CS system).\nWe are working towards dynamic, just-in-time partitioning of computation to utilize the extra compute resources available in a CS system. In this talk, I will describe how we approach just-in-time partitioning of computation by decomposing a logical query plan (initial query plan) into portions (subplans) that can be distributed. On any given CS device, a subplan is processed locally, translated to a physical query plan, then the physical query plan is executed. Execution of the physical query plan may be partial (best-effort), and the portions that were not executed are propagated with the query results so that CS devices higher in the data access path may, collectively, eventually complete execution of the initial query plan.\n","date":1692144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692144e3,"objectID":"2a9946ea928194f8594d73158066feac","permalink":"https://skyhookdm.github.io/talks/20230817/aldrin.montana/","publishdate":"2023-08-16T00:00:00Z","relpermalink":"/talks/20230817/aldrin.montana/","section":"talks","summary":"Existing research in computational storage primarily explores pushdown of a single query operator or its supporting functions from a database management system. In these cases, the portion of a query that can be pushed down is static, the execution of the kernel is device-specific, and the benefits of computational storage only exist between the database system and the compuational storage devices (CS devices) it is directly connected to.","tags":null,"title":"Query processing for a computational storage system","type":"talks"},{"authors":["Lokesh Jaliminche"],"categories":null,"content":"We have seen significant growth in data and data processing applications in recent years. For such applications, data movement has been one of the primary bottlenecks for performance and Energy consumption. Computational Storage Devices(CSDs) possess the potential to reduce data movement overheads by processing data in or near storage devices. However, we observe that their adaption has been slow because of the manual effort involved in analyzing applications to identify kernels that can be offloaded to CSDs. Existing methodologies follow an iterative implementation and evaluation cycle, which leads to a very slow design process and expensive iterations. In this talk, I will discuss our proposed methodology for application analysis and kernel identification with initial evaluation.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"d7beb668c8cd1a9b9e76b73de2e924f8","permalink":"https://skyhookdm.github.io/talks/20230817/lokesh.jaliminche/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/lokesh.jaliminche/","section":"talks","summary":"We have seen significant growth in data and data processing applications in recent years. For such applications, data movement has been one of the primary bottlenecks for performance and Energy consumption.","tags":null,"title":"A Methodology to Assist Kernel Offloading Decisions on Computational Storage","type":"talks"},{"authors":["Matthew Curry"],"categories":null,"content":"There are at least two methods for enabling computational storage workflows on embedded storage hardware. The first revolutionary path is to port applications to relatively lightweight storage abstractions that allow simple software stacks. A second evolutionary path is to enable existing storage abstractions by porting existing libraries and utilities to computational storage hardware. While the evolutionary path is a good stop-gap measure until applications can be prepared, porting significant dependency chains for complex libraries to the required interesting architectures can be a significant hurdle.\nIn this talk, I will describe the Advanced Tri-lab Software Environment (ATSE). ATSE is a traditional HPC software stack for testbed systems. As a full-featured stack, it is able to provide complex I/O libraries such as HDF5, NetCDF, CGNS, Exodus, and others. Crucially, ATSE has been demonstrated to port to many variants of Arm and RISC-V processors, allowing use on low-power hardware often found in computational storage devices. Along with the stack itself, this talk will touch on techniques and facilities allowing ATSE‚Äôs high levels of portability.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"6112b466a876fa8397ada0ed83347a69","permalink":"https://skyhookdm.github.io/talks/20230817/matthew.curry/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/matthew.curry/","section":"talks","summary":"There are at least two methods for enabling computational storage workflows on embedded storage hardware. The first revolutionary path is to port applications to relatively lightweight storage abstractions that allow simple software stacks.","tags":null,"title":"An HPC-Oriented Runtime Environment for Enabling Computational Storage","type":"talks"},{"authors":["Pankaj Mehra"],"categories":null,"content":"Simple enough to describe in one sentence, the theory of operation of XPI, an interface that targets acceleration starting to show up in and near memory and storage devices, revolves around three core concepts. First, an infrastructure abstraction that uniformizes disaggregated infrastructure. Second, a new abstraction for comprehending an application‚Äôs data spanning the full richness of modern memory and storage systems. Third, an abstraction capable of supporting the offloading of interdependent parallel computations with set-valued dependencies. The talk is motivated through the upcoming upheaval of memory hierarchy in data centers and concludes with a call to action for industry and academia participants alike.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"7881de4734178a2a8f198764267288d3","permalink":"https://skyhookdm.github.io/talks/20230817/pankaj.mehra/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/pankaj.mehra/","section":"talks","summary":"Simple enough to describe in one sentence, the theory of operation of XPI, an interface that targets acceleration starting to show up in and near memory and storage devices, revolves around three core concepts.","tags":null,"title":"Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface","type":"talks"},{"authors":["Craig Ulmer"],"categories":null,"content":"Advanced, scientific-computing workflows rely on composable data service libraries to migrate data between different simulation and analysis jobs that run in parallel on a high-performance computing (HPC) platform. While these services are essential for staging and transforming in-transit data, they consume compute node resources that could otherwise be used for scientific work. Recent programmable network interface cards (or SmartNICs) provide a new alternative for hosting services that enables data management tasks to run in an isolated space at the compute node that does not impact host resources. In this talk we explore extending composable data services into SmartNICs and describe a software stack for services that uses Faodel and Apache Arrow. To illustrate how this stack operates, we present a case study that implements a distributed, particle-sifting service for reorganizing simulation results. Performance experiments from a 100-node cluster equipped with 100Gb/s BlueField-2 SmartNICs indicate that current SmartNICs can perform useful data management tasks, albeit at a lower throughput than hosts.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"38f0ce1fc1d1cc00c159f6b553973177","permalink":"https://skyhookdm.github.io/talks/20230817/craigulmer/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/craigulmer/","section":"talks","summary":"Advanced, scientific-computing workflows rely on composable data service libraries to migrate data between different simulation and analysis jobs that run in parallel on a high-performance computing (HPC) platform. While these services are essential for staging and transforming in-transit data, they consume compute node resources that could otherwise be used for scientific work.","tags":null,"title":"Extending Composable Data Services into SmartNICs","type":"talks"},{"authors":["Carlos Maltzahn"],"categories":null,"content":"Open source software communities and their strategies and techniques has immense potential to amplify the impact of experimental systems research. The Center for Research in Open Source Software (cross.ucsc.edu) since I founded it in 2015 has successfully demonstrated this potential by raising $2.3 million in membership fees and creating the Skyhook Data Management incubator project that has now evolved into a community platform for addressing the challenges of the computational I/O stack and lowering barriers for industry to adopt solutions to these challenges. Some of the open source products of SkyhookDM research are now mature enough to require an open source ecosystem in order to facilitate their adoption. However much of the work to create that infrastructure is outside the job description of researchers. That is, we need staff software engineers and community architects to build this infrastructure.\nIn this talk I will give a quick overview of how CROSS v2 is planning to support paths to open source ecosystems by adapting its funding model, governance, and incubator.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"840b5fe527bb51832bf0aadada3ddf5c","permalink":"https://skyhookdm.github.io/talks/20230817/carlos.maltzahn/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/carlos.maltzahn/","section":"talks","summary":"Open source software communities and their strategies and techniques has immense potential to amplify the impact of experimental systems research. The Center for Research in Open Source Software (cross.ucsc.edu) since I founded it in 2015 has successfully demonstrated this potential by raising $2.","tags":null,"title":"Institutional Support for Experimental Systems Research","type":"talks"},{"authors":["Jianshen Liu"],"categories":null,"content":"High-performance computing (HPC) systems researchers have proposed using current, programmable network interface cards (or SmartNICs) to offload data management services that would otherwise consume host processor cycles in a platform. While this work has successfully mapped data pipelines to a collection of SmartNICs, users require a flexible means of inspecting in-transit data to assess the live state of the system. In this paper, we explore SmartNIC-driven opportunistic query execution, i.e., enabling the SmartNIC to make a decision about whether to execute a query operation locally (i.e., ‚Äúoffload‚Äù) or defer execution to the client (i.e., ‚Äúpush-back‚Äù). Characterizations of different parts of the end-to-end query path allow the decision engine to make complexity predictions that would not be feasible by the client alone.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"d8012aea30f8a025f135f6a11f7645e7","permalink":"https://skyhookdm.github.io/talks/20230817/jianshen.liu/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/jianshen.liu/","section":"talks","summary":"High-performance computing (HPC) systems researchers have proposed using current, programmable network interface cards (or SmartNICs) to offload data management services that would otherwise consume host processor cycles in a platform. While this work has successfully mapped data pipelines to a collection of SmartNICs, users require a flexible means of inspecting in-transit data to assess the live state of the system.","tags":null,"title":"Opportunistic Query Execution on SmartNICs for Analyzing In-Transit Data","type":"talks"},{"authors":["Esteban Ramos"],"categories":null,"content":"Remote procedure calls are a major contributor to performance variance in distributed systems due to lack of isolation and contention on shared resources. In this talk, I will introduce the novel architecture we built to address this problem, and cover our experience in implementing a prototype over gRPC with an emphasis on the communication layer between a host CPU and the SmartNIC. The basic idea is to partition RPC applications into two communicating components: one dedicated to user-implemented business logic, and one dedicated to RPC infrastructure processing. The infrastructure process can be run on a dedicated core or on a smart NIC (e.g., IPU or DPU), providing effective physical isolation and predictable performance. An evaluation of a proof-of-concept prototype shows that the split architecture adds modest overhead for average case latency, but allows for lower latency and and higher throughput under host CPU load.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"0f946a42052c656504a691de6a400d5e","permalink":"https://skyhookdm.github.io/talks/20230817/esteban.ramos/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/esteban.ramos/","section":"talks","summary":"Remote procedure calls are a major contributor to performance variance in distributed systems due to lack of isolation and contention on shared resources. In this talk, I will introduce the novel architecture we built to address this problem, and cover our experience in implementing a prototype over gRPC with an emphasis on the communication layer between a host CPU and the SmartNIC.","tags":null,"title":"Split gRPC: An Isolation Architecture for RPC Stacks on SmartNICs","type":"talks"},{"authors":["Jayjeet Chakraborty"],"categories":null,"content":"The amount of data stored in data centers worldwide is increasing faster than ever. Much of this data is stored to process, analyze, and extract valuable insights. Primarily, large datasets are stored in dumb storage servers inside data centers. To perform analysis and compute on this data, it must first be transported to compute servers, usually with big DRAMs and huge processing capabilities, where the data is processed and result sets sent to the client. Traditional data transport frameworks use TCP/IP as their underlying protocol and therefore require the users to provide with a single contiguous buffer for transfer to the framework. We observe that this approach adds a severe overhead when transferring batches of columnar records. Columnar tables consist of several buffers scattered in the memory, each representing a particular column and its metadata. Serializing these buffers into a single buffer requires doing multiple memory copies. To avoid this serialization overhead while transfer columnar batches over the wire, we propose using RDMA as a potential solution to this problem. In this paper, we explore transporting columnar result sets over RDMA from a server to a client and compare this approach with a state-of-the-art TCP/IP based transport framework. We show that using RDMA for columnar data transport provides up to 2x higher throughput.\n","date":1691712e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691712e3,"objectID":"33bcd6e7256cbce000a4d9c105f12678","permalink":"https://skyhookdm.github.io/talks/20230817/jayjeet.chakraborty/","publishdate":"2023-08-11T00:00:00Z","relpermalink":"/talks/20230817/jayjeet.chakraborty/","section":"talks","summary":"The amount of data stored in data centers worldwide is increasing faster than ever. Much of this data is stored to process, analyze, and extract valuable insights. Primarily, large datasets are stored in dumb storage servers inside data centers.","tags":null,"title":"Towards Faster Columnar Data Transport using RDMA","type":"talks"},{"authors":["Qing Zheng"],"categories":null,"content":"High-performance computing data centers supporting large-scale simulation applications can routinely generate a tremendous amount of data. For cost-effective high bandwidth, many data centers have used tiered storage with warmer tiers made of flashes and cooler tiers provisioned with high-density rotational disk drives. While recent advances in storage media, high-speed interconnects, and systems software have greatly improved modern platforms‚Äô ability to handle massive data, highly selective data retrievals, such as those in the form of SQL-like queries with predicates on potentially multiple data columns, tend to still experience excessive delays when unordered, unindexed data written in log-structured formats for high write bandwidth is subsequently read for interactive data analytics. Queries run slowly because an entire dataset may have to be transferred from storage to worker nodes, even when only a small portion is actually relevant. In the worst case, significant delays are experienced even when data is read from warm storage. A user sees even higher delays when data must be streamed from cool storage tiers.\nIn this presentation, we present C2, a research collaboration between Seagate and Los Alamos National Lab (LANL) for the lab‚Äôs next-generation campaign storage. Campaign is a scalable cool storage tier at LANL managed by MarFS that currently provides 100 PBs of storage space for long-term data storage. Cost-effective data protection is done through multi-level erasure coding at both node level and rack level. To prevent users from potentially having to read back an entire dataset, C2 enables direct SQL-like query processing at the storage level by leveraging Seagate Kinetic Computational Storage Drives for near data processing. Combining computational storage technologies with erasure coding based data protection schemes for rapid data analytics over cool storage presents unique challenges in which individual disk drives may not be able to see complete data records and may not deliver performance required by high-level data applications. We discuss those challenges in the talk, share our designs, and report early results. Our results show that computational storage shows higher performance when host-storage interconnection bandwidth is a bottleneck, when host CPU is a bottleneck, and, perhaps surprisingly, when hosts do not have any obvious bottlenecks.\n","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552e3,"objectID":"2a058d8de3151c76b81207c2e1bfb9e8","permalink":"https://skyhookdm.github.io/talks/20230817/qing.zheng/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/talks/20230817/qing.zheng/","section":"talks","summary":"High-performance computing data centers supporting large-scale simulation applications can routinely generate a tremendous amount of data. For cost-effective high bandwidth, many data centers have used tiered storage with warmer tiers made of flashes and cooler tiers provisioned with high-density rotational disk drives.","tags":null,"title":"C2: LANL-Seagate's Early Prototype for Near-Data, SQL-Like Query Processing","type":"talks"},{"authors":["Carlos Maltzahn"],"categories":null,"content":"üëã Join us for an exciting event featuring IEEE Distinguished Lecturer Yoichiro Tanaka (Tohoku University) discussing technological and institutional innovations to make the computational I/O stack a reality!\nRegister ‚Äì\u0026gt; The event is hybrid ‚Äì we highly encourage in-person attendance and welcome remote participation. The introduction of computational data management services into the I/O stack, especially in storage and networking devices, requires both technological innovations and new relations between university and industry. This one-day workshop will convene experts from storage systems, open source, and community architecture to discuss technologies and strategies for a computational I/O stack with low market entry barriers.\nThe workshop will take place on August 17, 2023 from 10am to 5pm, at UC Santa Cruz, Engineering 2, Room 506 (5th floor, north-west of the lobby/elevators, see floor plans), and is jointly organized by the IEEE Magnetics Society‚Äôs Distinguished Lecturers Program, the Skyhook Data Management community with funding by the National Science Foundation (TI-2229773), the Center for Research in Open Source Software (cross.ucsc.edu), and the Open Source Program Office, UC Santa Cruz (ospo.ucsc.edu).\nRegister ‚Äì\u0026gt; Time Title Speaker(s) 10-10:15am Introduction Carlos Maltzahn (UC Santa Cruz), Philip Kufeldt 10:15-noon Session 1 10:15-11:15am IEEE Distinguished Lecture: Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration Yoichiro Tanaka (Tohoku University) 11:15-11:30am Q\u0026amp;A and Discussion 11:30-11:45am Institutional Support for Experimental Systems Research Carlos Maltzahn (UC Santa Cruz) 11:45-noon Q\u0026amp;A and Discussion noon-1pm Lunch (provided) 1-2:05pm Session 2 1-1:20pm Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface Pankaj Mehra (UCSC, Elephance Memory) 1:20-1:35pm Apache Arrow Ian Cook (Voltron Data) 1:35-1:50pm Substrait - Engine \u0026amp; Language Independent Data Compute Instructions (slides) Jacques Nadeau (Sundeck) 1:50-2:05pm Kinetic Open Storage Update Philip Kufeldt 2:05-2:20pm Break (refreshments provided) 2:20-3.05pm Session 3 2:20-2:35pm Query processing for a computational storage system Aldrin Montana (UC Santa Cruz) 2:35-3:05pm C2: LANL-Seagate‚Äôs Early Prototype for Near-Data, SQL-Like Query Processing Qing Zheng (Los Alamos National Lab) 3:05-3:20pm Break (refreshments provided) 3:20-4:10pm Session 4 3:20-3:40pm Extending Composable Data Services into SmartNICS Craig Ulmer (Sandia National Labs) 3:40-3:55pm Opportunistic Query Execution on SmartNICs for Analyzing In-Transit Data Jianshen Liu (UC Santa Cruz) 3:55-4:10pm An HPC-Oriented Runtime Environment for Enabling Computational Storage Matthew L. Curry (Sandia National Labs) 4:10-4:25pm Break (refreshments provided) 4:25-5pm Session 5 4:25-4:40pm Towards Faster Columnar Data Transport using RDMA Jayjeet Chakraborty (UC Santa Cruz) 4:40-4:50 A Methodology to Assist Kernel Offloading Decisions on Computational Storage Lokesh Jaliminche (UC Santa Cruz) 4:50-5pm Split gRPC: An Isolation Architecture for RPC Stacks on SmartNICs Esteban Ramos (UC Santa Cruz) Table: Agenda Register ‚Äì\u0026gt; ","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552e3,"objectID":"039125c9423a97146763e71999b88f58","permalink":"https://skyhookdm.github.io/post/20230718-aug17/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/post/20230718-aug17/","section":"post","summary":"A workshop featuring keynote speaker Yoichiro Tanaka (Tohoku University) that will take place from 10am to 3pm (PT), at UC Santa Cruz in the Engineering 2 Building, Room 506 (5th floor, north-west of the lobby/elevators, see [floor plans](https://facilities.soe.ucsc.edu/floor-plans)).","tags":null,"title":"Computational I/O Stack Workshop, August 17, 2023","type":"post"},{"authors":["Philip Kufeldt"],"categories":null,"content":"Prior to departing Seagate I was able to open source the remaining components of Seagate‚Äôs Kinetic implementation, namely their server, kineticd. This kinetic server implementation was used inside their production 4T and 8T Kinetic HDD offerings. An entire Kinetic research vehicle in source form is now available for research, from client to server. This presentation will go over this open source.\n","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552e3,"objectID":"dce024aba13a3dfa11f3045119b7a0c1","permalink":"https://skyhookdm.github.io/talks/20230817/philip.kufeldt/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/talks/20230817/philip.kufeldt/","section":"talks","summary":"Prior to departing Seagate I was able to open source the remaining components of Seagate‚Äôs Kinetic implementation, namely their server, kineticd. This kinetic server implementation was used inside their production 4T and 8T Kinetic HDD offerings.","tags":null,"title":"Kinetic Open Storage Project Update","type":"talks"},{"authors":["Yoichiro Tanaka"],"categories":null,"content":"The digital world is producing nearly a hundred Zetta bytes of data per year and creating values for the quality of society. A huge amount of data is being stored, processed, transmitted, and then shared via large scale networked datacenters which consist of millions of data storage systems filled with perpendicular magnetic recording (PMR) hard disk drives. The PMR technology was invented by Shunichi Iwasaki in 1975 and the first commercial product was launched in 2005. Since then, the data storage has become the ever-growing foundation of the digital world and led the data-driven innovations such as bigdata AI analytics, internet of things, medical science, and even a blackhole visualization in astronomy.\nThis lecture will provide the essential magnetics to create innovative data storage technology of PMR and the origin of the high-density recording performance which has led current recording density growth. The storage performance stands on the stacked system foundation and the building blocks are, from the base, physics of magnetics, 3D material controls of sub-nanometer in size, magnetic and electronic device design, storage device integration, and storage system architectures together with non-volatile memories to unleash the intrinsic performance. The development of new storage devices and the system requires a multi-scale approach and a right guiding principle to establish expected functions. As an extension of PMR research, the lecture will also show the prospect of future storage technology and the system architecture from the multi-scale view of the storage system development. A new computational storage system aiming at unifying computation power on data store and brain-inspired system considerations as well as the academism-industry relations to realize those systems will also be introduced.\n","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689552e3,"objectID":"76ddab8993ff1fc9b141e3a895fe4670","permalink":"https://skyhookdm.github.io/talks/20230817/yoichiro.tanaka/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/talks/20230817/yoichiro.tanaka/","section":"talks","summary":"The digital world is producing nearly a hundred Zetta bytes of data per year and creating values for the quality of society. A huge amount of data is being stored, processed, transmitted, and then shared via large scale networked datacenters which consist of millions of data storage systems filled with perpendicular magnetic recording (PMR) hard disk drives.","tags":null,"title":"Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration","type":"talks"},{"authors":["Craig Ulmer","Jianshen Liu","Carlos Maltzahn","Matthew L. Curry"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"2c118178960eeba9b4a0adab34e2539b","permalink":"https://skyhookdm.github.io/publication/ulmer-compsys-23/","publishdate":"2023-06-08T01:20:50.634899Z","relpermalink":"/publication/ulmer-compsys-23/","section":"publication","summary":"Advanced scientific-computing workflows rely on composable data services to migrate data between simulation and analysis jobs that run in parallel on high-performance computing (HPC) platforms. Unfortunately, these services consume compute-node memory and processing resources that could otherwise be used to complete the workflow‚Äôs tasks. The emergence of programmable network interface cards, or SmartNICs, presents an opportunity to host data services in an isolated space within a compute node that does not impact host resources. In this paper we explore extending data services into SmartNICs and describe a software stack for services that uses Faodel and Apache Arrow. To illustrate how this stack operates, we present a case study that implements a distributed, particle-sifting service for reorganizing simulation results. Performance experiments from a 100-node cluster equipped with 100Gb/s BlueField-2 SmartNICs indicate that current SmartNICs can perform useful data management tasks, albeit at a lower throughput than hosts.","tags":["smartnics","composability","datamanagement"],"title":"Extending Composable Data Services into SmartNICS (Best Paper Award)","type":"publication"},{"authors":["Jianshen Liu","Carlos Maltzahn","Matthew L. Curry","Craig Ulmer"],"categories":[],"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660699385,"objectID":"97962b126e6f272e3726162f17fd8f88","permalink":"https://skyhookdm.github.io/publication/liu-hpec-22/","publishdate":"2022-08-17T01:23:04.962887Z","relpermalink":"/publication/liu-hpec-22/","section":"publication","summary":"Many distributed applications implement complex data flows and need a flexible mechanism for routing data between producers and consumers. Recent advances in programmable network interface cards, or SmartNICs, represent an opportunity to offload data-flow tasks into the network fabric, thereby freeing the hosts to perform other work. System architects in this space face multiple questions about the best way to leverage SmartNICs as processing elements in data flows. In this paper, we advocate the use of Apache Arrow as a foundation to implement data flow tasks on SmartNICs. We report on our experience adapting a partitioning algorithm for particle data to Apache Arrow and measure the on-card processing performance for the BlueField-2 SmartNIC. Our experiments confirm that the BlueField-2's (de)compression hardware can have a significant impact on in-transit workflows where data must be unpacked, processed, and repacked.","tags":["smartnics","offloading","datamanagement","hpc"],"title":"Processing Particle Data Flows with SmartNICs (Outstanding Student Paper)","type":"publication"},{"authors":["Jayjeet Chakraborty","Ivo Jimenez","Sebastiaan Alvarez Rodriguez","Alexandru Uta","Jeff LeFevre","Carlos Maltzahn"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650924479,"objectID":"c94e7367e6c8ebf718d751375b36023b","permalink":"https://skyhookdm.github.io/publication/chakraborty-ccgrid-22/","publishdate":"2022-04-25T22:07:44.206228Z","relpermalink":"/publication/chakraborty-ccgrid-22/","section":"publication","summary":"With the ever-increasing dataset sizes, several file formats such as Parquet, ORC, and Avro have been developed to store data efficiently, save the network, and interconnect bandwidth at the price of additional CPU utilization. However, with the advent of networks supporting 25-100 Gb/s and storage devices delivering 1,000,000 reqs/sec, the CPU has become the bottleneck trying to keep up feeding data in and out of these fast devices. The result is that data access libraries executed on single clients are often CPU-bound and cannot utilize the scale-out benefits of distributed storage systems. One attractive solution to this problem is to offload data-reducing processing and filtering tasks to the storage layer. However, modifying legacy storage systems to support compute offloading is often tedious and requires an extensive understanding of the system internals. Previous approaches re-implemented functionality of data processing frameworks and access libraries for a particular storage system, a duplication of effort that might have to be repeated for different storage systems.  This paper introduces a new design paradigm that allows extending programmable object storage systems to embed existing, widely used data processing frameworks and access libraries into the storage layer with no modifications. In this approach, data processing frameworks and access libraries can evolve independently from storage systems while leveraging distributed storage systems' scale-out and availability properties. We present Skyhook, an example implementation of our design paradigm using Ceph, Apache Arrow, and Parquet. We provide a brief performance evaluation of Skyhook and discuss key results.","tags":["papers","programmable","storage","systems","arrow","nsf1836650","nsf1705021","nsf1764102"],"title":"Skyhook: Towards an Arrow-Native Storage System","type":"publication"},{"authors":["Jayjeet Chakraborty","Carlos Maltzahn","David Li","Tom Drabas"],"categories":[],"content":"","date":1643587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688515200,"objectID":"c05a2dfe467c799ab0607b586ff94793","permalink":"https://skyhookdm.github.io/publication/chakraborty-arrowblog-22/","publishdate":"2022-05-08T18:22:08.329823Z","relpermalink":"/publication/chakraborty-arrowblog-22/","section":"publication","summary":"CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they‚Äôre improving in different dimensions. Processors are faster, but their memory bandwidth hasn‚Äôt kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link. This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\nFor example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client. Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter. While formats like Apache Parquet enable some optimizations, fundamentally, the responsibility is all on the client. Meanwhile, even though the storage system has its own compute capabilities, it‚Äôs relegated to just serving ‚Äúdumb bytes‚Äù.\nThanks to the [Center for Research in Open Source Software](https://cross.ucsc.edu) (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets](https://arrow.apache.org/docs/cpp/dataset.html) extension that solves this problem by using the storage layer to reduce client resource utilization. We‚Äôll examine the developments surrounding Skyhook as well as how Skyhook works.","tags":["computation","storage","programmable","datamanagement","ceph","arrow"],"title":" Skyhook: Bringing Computation to Storage with Apache Arrow ","type":"publication"},{"authors":["Sebastiaan Alvarez Rodriguez","Jayjeet Chakraborty","Aaron Chu","Ivo Jimenez","Jeff LeFevre","Carlos Maltzahn","Alexandru Uta"],"categories":[],"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650924479,"objectID":"efc006628dfcfb689d00a92ffbcf4939","permalink":"https://skyhookdm.github.io/publication/rodriguez-bigdata-21/","publishdate":"2022-04-25T22:07:59.443656Z","relpermalink":"/publication/rodriguez-bigdata-21/","section":"publication","summary":"Distributed data processing ecosystems are widespread and their components are highly specialized, such that efficient interoperability is urgent. Recently, Apache Arrow was chosen by the community to serve as a format mediator, providing efficient in-memory data representation. Arrow enables efficient data movement between data processing and storage engines, significantly improving interoperability and overall performance. In this work, we design a new zero-cost data interoperability layer between Apache Spark and Arrow-based data sources through the Arrow Dataset API. Our novel data interface helps separate the computation (Spark) and data (Arrow) layers. This enables practitioners to seamlessly use Spark to access data from all Arrow Dataset API-enabled data sources and frameworks. To benefit our community, we open-source our work and show that consuming data through Apache Arrow is zero-cost: our novel data interface is either on-par or more performant than native Spark.","tags":["papers","spark","arrow","performance","nsf1836650"],"title":"Zero-Cost, Arrow-Enabled Data Interface for Apache Spark","type":"publication"},{"authors":["Aaron Chu","Jeff LeFevre","Carlos Maltzahn","Aldrin Montana","Peter Alvaro","Dana Robinson","Quincey Koziol"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"fea189160dee93ce7424cc48371e9981","permalink":"https://skyhookdm.github.io/publication/chu-epjconf-20/","publishdate":"2021-02-21T00:24:01.226701Z","relpermalink":"/publication/chu-epjconf-20/","section":"publication","summary":"Access libraries such as ROOT and HDF5 allow users to interact with datasets using high level abstractions, like coordinate systems and associated slicing operations. Unfortunately, the implementations of access libraries are based on outdated assumptions about storage systems interfaces and are generally unable to fully benefit from modern fast storage devices. For example, access libraries often implement buffering and data layout that assume that large, single-threaded sequential access patterns are causing less overall latency than small parallel random access: while this is true for spinning media, it is not true for flash media. The situation is getting worse with rapidly evolving storage devices such as non-volatile memory and ever larger datasets. Our Skyhook Dataset Mapping project explores distributed dataset mapping infrastructures that can integrate and scale out existing access libraries using Ceph's extensible object model, avoiding reimplementation or even modifications of these access libraries as much as possible. These programmable storage extensions coupled with our distributed dataset mapping techniques enable: 1) access library operations to be offloaded to storage system servers, 2) the independent evolution of access libraries and storage systems and 3) fully leveraging of the existing load balancing, elasticity, and failure management of distributed storage systems like Ceph. They also create more opportunities to conduct storage server-local optimizations specific to storage servers. For example, storage servers might include local key/value stores combined with chunk stores that require different optimizations than a local file system. As storage servers evolve to support new storage devices like non-volatile memory, these server-local optimizations can be implemented while minimizing disruptions to applications. We will report progress on the means by which distributed dataset mapping can be abstracted over particular access libraries, including access libraries for ROOT data, and how we address some of the challenges revolving around data partitioning and composability of access operations.","tags":["papers","programmable","declarative","objectstorage","nsf1836650"],"title":"Mapping Scientific Datasets to Programmable Storage","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"865e411be877820cdabae9f09315eb30","permalink":"https://skyhookdm.github.io/publication/lefevre-snia-20/","publishdate":"2023-01-26T14:23:16.862545Z","relpermalink":"/publication/lefevre-snia-20/","section":"publication","summary":"","tags":["programmable","storage"],"title":"SkyhookDM: Storage and Management of Tabular Data in Ceph","type":"publication"},{"authors":["Jianshen Liu","Matthew Leon Curry","Carlos Maltzahn","Philip Kufeldt"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f7340b531adf3e2f68f81ac006cc8e75","permalink":"https://skyhookdm.github.io/publication/liu-hotedge-20/","publishdate":"2020-04-19T19:42:33.148866Z","relpermalink":"/publication/liu-hotedge-20/","section":"publication","summary":"In the resource-rich environment of data centers most failures can quickly failover to redundant resources. In contrast, failure in edge infrastructures with limited resources might require maintenance personnel to drive to the location in order to fix the problem. The operational cost of these 'truck rolls' to locations at the edge infrastructure competes with the operational cost incurred by extra space and power needed for redundant resources at the edge. Computational storage devices with network interfaces can act as network-attached storage servers and offer a new design point for storage systems at the edge. In this paper we hypothesize that a system consisting of a larger number of such small 'embedded' storage nodes provides higher availability due to a larger number of failure domains while also saving operational cost in terms of space and power. As evidence for our hypothesis, we compared the possibility of data loss between two different types of storage systems: one is constructed with general-purpose servers, and the other one is constructed with embedded storage nodes. Our results show that the storage system constructed with general-purpose servers has 7 to 20 times higher risk of losing data over the storage system constructed with embedded storage devices. We also compare the two alternatives in terms of power and space using the Media-Based Working Unit (MBWU) that we developed in an earlier paper as a reference point.","tags":["papers","edge","reliability","disaggregation","embedded","failures"],"title":"Scale-out Edge Storage Systems with Embedded Storage Nodes to Get Better Availability and Cost-Efficiency At the Same Time","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":159192e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":159192e4,"objectID":"915e552d840309dc5886f66457e45fc1","permalink":"https://skyhookdm.github.io/publication/lefevre-login-20/","publishdate":"2020-06-13T01:39:15.018368Z","relpermalink":"/publication/lefevre-login-20/","section":"publication","summary":"","tags":["papers","programmable","storage","ceph","physicaldesign"],"title":"SkyhookDM: Data Processing in Ceph with Programmable Storage","type":"publication"},{"authors":["Jeff LeFevre","Carlos Maltzahn"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"c0997a47ffd719cd2ea0c0d4f82ae9a9","permalink":"https://skyhookdm.github.io/publication/lefevre-vault-20/","publishdate":"2020-01-05T06:43:50.391599Z","relpermalink":"/publication/lefevre-vault-20/","section":"publication","summary":"","tags":["shortpapers","programmable","storage","physicaldesign"],"title":"Scaling databases and file APIs with programmable Ceph object storage","type":"publication"},{"authors":["Kathryn Dahlgren","Jeff LeFevre","Ashay Shirwadkar","Ken Iizawa","Aldrin Montana","Peter Alvaro","Carlos Maltzahn"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"e615944dd1d3f72bced85e81bb5a0490","permalink":"https://skyhookdm.github.io/publication/dahlgren-pdsw-19/","publishdate":"2020-01-05T06:43:50.411373Z","relpermalink":"/publication/dahlgren-pdsw-19/","section":"publication","summary":"In the post-Moore era, systems and devices with new architectures will arrive at a rapid rate with significant impacts on the software stack. Applications will not be able to fully benefit from new architectures unless they can delegate adapting to new devices in lower layers of the stack. In this paper we introduce physical design management which deals with the problem of identifying and executing transformations on physical designs of stored data, i.e. how data is mapped to storage abstractions like files, objects, or blocks, in order to improve performance. Physical design is traditionally placed with applications, access libraries, and databases, using hard- wired assumptions about underlying storage systems. Yet, storage systems increasingly not only contain multiple kinds of storage devices with vastly different performance profiles but also move data among those storage devices, thereby changing the benefit of a particular physical design. We advocate placing physical design management in storage, identify interesting research challenges, provide a brief description of a prototype implementation in Ceph, and discuss the results of initial experiments at scale that are replicable using Cloudlab. These experiments show performance and resource utilization trade-offs associated with choosing different physical designs and choosing to transform between physical designs.","tags":["papers","programmable","storage","datamanagement","physicaldesign"],"title":"Towards Physical Design Management in Storage Systems","type":"publication"},{"authors":["Jeff LeFevre","Noah Watkins","Michael Sevilla","Carlos Maltzahn"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"a06d72e4b291d58378ac21f0662aef77","permalink":"https://skyhookdm.github.io/publication/lefevre-vault-19/","publishdate":"2020-01-05T06:43:50.416084Z","relpermalink":"/publication/lefevre-vault-19/","section":"publication","summary":"Ceph is an open source distributed storage system that is object-based and massively scalable. Ceph provides developers with the capability to create data interfaces that can take advantage of local CPU and memory on the storage nodes (Ceph Object Storage Devices). These interfaces are powerful for application developers and can be created in C, C++, and Lua.\nSkyhook is an open source storage and database project in the Center for Research in Open Source Software at UC Santa Cruz. Skyhook uses these capabilities in Ceph to create specialized read/write interfaces that leverage IO and CPU within the storage layer toward database processing and management. Specifically, we develop methods to apply predicates locally as well as additional metadata and indexing capabilities using Ceph's internal indexing mechanism built on top of RocksDB.\nSkyhook's approach helps to enable scale-out of a single node database system by scaling out the storage layer. Our results show the performance benefits for some queries indeed scale well as the storage layer scales out.","tags":["papers","programmable","storage","database"],"title":"Skyhook: Programmable storage for databases","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://skyhookdm.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]