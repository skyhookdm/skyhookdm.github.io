<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Talks | Skyhook Data Management</title><link>https://skyhookdm.github.io/talks/</link><atom:link href="https://skyhookdm.github.io/talks/index.xml" rel="self" type="application/rss+xml"/><description>Talks</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 11 Aug 2023 00:00:00 +0000</lastBuildDate><image><url>https://skyhookdm.github.io/media/logo_hu5cc6dc8723ec228d8c1f642a46eb1326_91430_300x300_fit_lanczos_3.png</url><title>Talks</title><link>https://skyhookdm.github.io/talks/</link></image><item><title>An HPC-Oriented Runtime Environment for Enabling Computational Storage</title><link>https://skyhookdm.github.io/talks/20230817/matthew.curry/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/matthew.curry/</guid><description>&lt;p>There are at least two methods for enabling computational storage workflows on embedded storage hardware. The first revolutionary path is to port applications to relatively lightweight storage abstractions that allow simple software stacks. A second evolutionary path is to enable existing storage abstractions by porting existing libraries and utilities to computational storage hardware. While the evolutionary path is a good stop-gap measure until applications can be prepared, porting significant dependency chains for complex libraries to the required interesting architectures can be a significant hurdle.&lt;/p>
&lt;p>In this talk, I will describe the Advanced Tri-lab Software Environment (ATSE). ATSE is a traditional HPC software stack for testbed systems. As a full-featured stack, it is able to provide complex I/O libraries such as HDF5, NetCDF, CGNS, Exodus, and others. Crucially, ATSE has been demonstrated to port to many variants of Arm and RISC-V processors, allowing use on low-power hardware often found in computational storage devices. Along with the stack itself, this talk will touch on techniques and facilities allowing ATSE’s high levels of portability.&lt;/p></description></item><item><title>Contemplating a new Compute-Memory Hierarchy through the lens of an Acceleration Programming Interface</title><link>https://skyhookdm.github.io/talks/20230817/pankaj.mehra/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/pankaj.mehra/</guid><description>&lt;p>Simple enough to describe in one sentence, the theory of operation of XPI, an interface that targets acceleration starting to show up in and near memory and storage devices, revolves around three core concepts. First, an infrastructure abstraction that uniformizes disaggregated infrastructure. Second, a new abstraction for comprehending an application&amp;rsquo;s data spanning the full richness of modern memory and storage systems. Third, an abstraction capable of supporting the offloading of interdependent parallel computations with set-valued dependencies. The talk is motivated through the upcoming upheaval of memory hierarchy in data centers and concludes with a call to action for industry and academia participants alike.&lt;/p></description></item><item><title>Extending Composable Data Services into SmartNICs</title><link>https://skyhookdm.github.io/talks/20230817/craigulmer/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/craigulmer/</guid><description>&lt;p>Advanced, scientific-computing workflows rely on composable data service libraries to migrate data between different simulation and analysis jobs that run in parallel on a high-performance computing (HPC) platform. While these services are essential for staging and transforming in-transit data, they consume compute node resources that could otherwise be used for scientific work. Recent programmable network interface cards (or SmartNICs) provide a new alternative for hosting services that enables data management tasks to run in an isolated space at the compute node that does not impact host resources. In this talk we explore extending composable data services into SmartNICs and describe a software stack for services that uses Faodel and Apache Arrow. To illustrate how this stack operates, we present a case study that implements a distributed, particle-sifting service for reorganizing simulation results. Performance experiments from a 100-node cluster equipped with 100Gb/s BlueField-2 SmartNICs indicate that current SmartNICs can perform useful data management tasks, albeit at a lower throughput than hosts.&lt;/p></description></item><item><title>Institutional Support for Experimental Systems Research</title><link>https://skyhookdm.github.io/talks/20230817/carlos.maltzahn/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/carlos.maltzahn/</guid><description>&lt;p>Open source software communities and their strategies and techniques has immense potential to amplify the impact of experimental systems research. The Center for Research in Open Source Software (&lt;a href="https://cross.ucsc.edu" target="_blank" rel="noopener">cross.ucsc.edu&lt;/a>) since I founded it in 2015 has successfully demonstrated this potential by raising $2.3 million in membership fees and creating the Skyhook Data Management incubator project that has now evolved into a community platform for addressing the challenges of the computational I/O stack and lowering barriers for industry to adopt solutions to these challenges. Some of the open source products of SkyhookDM research are now mature enough to require an open source ecosystem in order to facilitate their adoption. However much of the work to create that infrastructure is outside the job description of researchers. That is, we need staff software engineers and community architects to build this infrastructure.&lt;/p>
&lt;p>In this talk I will give a quick overview of how CROSS v2 is planning to support paths to open source ecosystems by adapting its funding model, governance, and incubator.&lt;/p></description></item><item><title>Split gRPC: An Isolation Architecture for RPC Stacks on SmartNICs</title><link>https://skyhookdm.github.io/talks/20230817/esteban.ramos/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/esteban.ramos/</guid><description>&lt;p>Remote procedure calls are a major contributor to performance variance in distributed systems due to lack of isolation and contention on shared resources. In this talk, I will introduce the novel architecture we built to address this problem, and cover our experience in implementing a prototype over gRPC with an emphasis on the communication layer between a host CPU and the SmartNIC. The basic idea is to partition RPC applications into two communicating components: one dedicated to user-implemented business logic, and one dedicated to RPC infrastructure processing. The infrastructure process can be run on a dedicated core or on a smart NIC (e.g., IPU or DPU), providing effective physical isolation and predictable performance. An evaluation of a proof-of-concept prototype shows that the split architecture adds modest overhead for average case latency, but allows for lower latency and and higher throughput under host CPU load.&lt;/p></description></item><item><title>Towards Faster Columnar Data Transport using RDMA</title><link>https://skyhookdm.github.io/talks/20230817/jayjeet.chakraborty/</link><pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/jayjeet.chakraborty/</guid><description>&lt;p>The amount of data stored in data centers worldwide is increasing faster than ever. Much of this data is stored to process, analyze, and extract valuable insights. Primarily, large datasets are stored in dumb storage servers inside data centers. To perform analysis and compute on this data, it must first be transported to compute servers, usually with big DRAMs and huge processing capabilities, where the data is processed and result sets sent to the client. Traditional data transport frameworks use TCP/IP as their underlying protocol and therefore require the users to provide with a single contiguous buffer for transfer to the framework. We observe that this approach adds a severe overhead when transferring batches of columnar records. Columnar tables consist of several buffers scattered in the memory, each representing a particular column and its metadata. Serializing these buffers into a single buffer requires doing multiple memory copies. To avoid this serialization overhead while transfer columnar batches over the wire, we propose using RDMA as a potential solution to this problem. In this paper, we explore transporting columnar result sets over RDMA from a server to a client and compare this approach with a state-of-the-art TCP/IP based transport framework. We show that using RDMA for columnar data transport provides up to 2x higher throughput.&lt;/p></description></item><item><title>C2: LANL-Seagate's Early Prototype for Near-Data, SQL-Like Query Processing</title><link>https://skyhookdm.github.io/talks/20230817/qing.zheng/</link><pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/qing.zheng/</guid><description>&lt;p>High-performance computing data centers supporting large-scale simulation applications can routinely generate a tremendous amount of data. For cost-effective high bandwidth, many data centers have used tiered storage with warmer tiers made of flashes and cooler tiers provisioned with high-density rotational disk drives. While recent advances in storage media, high-speed interconnects, and systems software have greatly improved modern platforms’ ability to handle massive data, highly selective data retrievals, such as those in the form of SQL-like queries with predicates on potentially multiple data columns, tend to still experience excessive delays when unordered, unindexed data written in log-structured formats for high write bandwidth is subsequently read for interactive data analytics. Queries run slowly because an entire dataset may have to be transferred from storage to worker nodes, even when only a small portion is actually relevant. In the worst case, significant delays are experienced even when data is read from warm storage. A user sees even higher delays when data must be streamed from cool storage tiers.&lt;/p>
&lt;p>In this presentation, we present C2, a research collaboration between Seagate and Los Alamos National Lab (LANL) for the lab&amp;rsquo;s next-generation campaign storage. Campaign is a scalable cool storage tier at LANL managed by MarFS that currently provides 100 PBs of storage space for long-term data storage. Cost-effective data protection is done through multi-level erasure coding at both node level and rack level. To prevent users from potentially having to read back an entire dataset, C2 enables direct SQL-like query processing at the storage level by leveraging Seagate Kinetic Computational Storage Drives for near data processing. Combining computational storage technologies with erasure coding based data protection schemes for rapid data analytics over cool storage presents unique challenges in which individual disk drives may not be able to see complete data records and may not deliver performance required by high-level data applications. We discuss those challenges in the talk, share our designs, and report early results. Our results show that computational storage shows higher performance when host-storage interconnection bandwidth is a bottleneck, when host CPU is a bottleneck, and, perhaps surprisingly, when hosts do not have any obvious bottlenecks.&lt;/p></description></item><item><title>Kinetic Open Storage Project Update</title><link>https://skyhookdm.github.io/talks/20230817/philip.kufeldt/</link><pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/philip.kufeldt/</guid><description>&lt;p>Prior to departing Seagate I was able to open source the remaining components of Seagate&amp;rsquo;s Kinetic implementation, namely their server, kineticd. This kinetic server implementation was used inside their production 4T and 8T Kinetic HDD offerings. An entire Kinetic research vehicle in source form is now available for research, from client to server. This presentation will go over this open source.&lt;/p></description></item><item><title>Magnetic data storage technology; from the invention of perpendicular magnetic recording (PMR) to the social integration</title><link>https://skyhookdm.github.io/talks/20230817/yoichiro.tanaka/</link><pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate><guid>https://skyhookdm.github.io/talks/20230817/yoichiro.tanaka/</guid><description>&lt;p>The digital world is producing nearly a hundred Zetta bytes of data per year and creating values for the quality of society. A huge amount of data is being stored, processed, transmitted, and then shared via large scale networked datacenters which consist of millions of data storage systems filled with perpendicular magnetic recording (PMR) hard disk drives. The PMR technology was invented by Shunichi Iwasaki in 1975 and the first commercial product was launched in 2005. Since then, the data storage has become the ever-growing foundation of the digital world and led the data-driven innovations such as bigdata AI analytics, internet of things, medical science, and even a blackhole visualization in astronomy.&lt;/p>
&lt;p>This lecture will provide the essential magnetics to create innovative data storage technology of PMR and the origin of the high-density recording performance which has led current recording density growth. The storage performance stands on the stacked system foundation and the building blocks are, from the base, physics of magnetics, 3D material controls of sub-nanometer in size, magnetic and electronic device design, storage device integration, and storage system architectures together with non-volatile memories to unleash the intrinsic performance. The development of new storage devices and the system requires a multi-scale approach and a right guiding principle to establish expected functions. As an extension of PMR research, the lecture will also show the prospect of future storage technology and the system architecture from the multi-scale view of the storage system development. A new computational storage system aiming at unifying computation power on data store and brain-inspired system considerations as well as the academism-industry relations to realize those systems will also be introduced.&lt;/p></description></item></channel></rss>